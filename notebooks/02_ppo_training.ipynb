{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLYFhLBeUbC-"
      },
      "source": [
        "# PPO Training for Dermatology Triage Clinic\n",
        "\n",
        "**Reinforcement Learning Summative Assignment**\n",
        "\n",
        "This notebook trains a Proximal Policy Optimization (PPO) agent on the custom dermatology clinic triage environment.\n",
        "\n",
        "## Workflow:\n",
        "1. **Setup**: Mount Drive, install dependencies\n",
        "2. **Environment**: Embed ClinicEnv code\n",
        "3. **Configurations**: Load 10 PPO hyperparameter configs\n",
        "4. **Quick Sweep**: Train all 10 configs for 50K steps each\n",
        "5. **Analysis**: Identify best performing configuration\n",
        "6. **Full Training**: Train best config with 5 seeds for 200K steps\n",
        "7. **Evaluation**: Generate plots and performance metrics\n",
        "8. **Export**: Save models and results to Google Drive\n",
        "\n",
        "**Estimated Runtime**: 5-7 hours on Colab GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28WvaIJ4UbDA"
      },
      "source": [
        "## 1. Setup: Mount Google Drive & Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "77wxQwTsUbDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37991885-8e45-4ada-ecff-f768bff079e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úì Google Drive mounted\n",
            "‚úì Project directory: /content/drive/MyDrive/RL_Summative\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up project directory on Drive\n",
        "import os\n",
        "PROJECT_DIR = '/content/drive/MyDrive/RL_Summative'\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "os.makedirs(f'{PROJECT_DIR}/models/ppo', exist_ok=True)\n",
        "os.makedirs(f'{PROJECT_DIR}/logs/ppo', exist_ok=True)\n",
        "os.makedirs(f'{PROJECT_DIR}/results', exist_ok=True)\n",
        "os.makedirs(f'{PROJECT_DIR}/plots', exist_ok=True)\n",
        "\n",
        "print(f\"‚úì Google Drive mounted\")\n",
        "print(f\"‚úì Project directory: {PROJECT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cszla5cIUbDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea0c37b-905f-4b50-c0cc-24f0d833d29e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "‚úì All packages installed successfully!\n",
            "============================================================\n",
            "\n",
            "‚ö†Ô∏è  IMPORTANT: RESTART RUNTIME NOW\n",
            "Click 'Runtime' ‚Üí 'Restart runtime' in the menu above\n",
            "Then run ALL cells again from the beginning\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# NOTE: numpy 1.26.4 is specified to avoid binary incompatibility with torch\n",
        "# Install numpy FIRST, then restart runtime before importing any libraries\n",
        "%pip install -q numpy==1.26.4\n",
        "%pip install -q torch\n",
        "%pip install -q gymnasium\n",
        "%pip install -q stable-baselines3\n",
        "%pip install -q sb3-contrib\n",
        "%pip install -q matplotlib\n",
        "%pip install -q seaborn\n",
        "%pip install -q pandas\n",
        "%pip install -q tqdm\n",
        "%pip install -q imageio\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úì All packages installed successfully!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT: RESTART RUNTIME NOW\")\n",
        "print(\"Click 'Runtime' ‚Üí 'Restart runtime' in the menu above\")\n",
        "print(\"Then run ALL cells again from the beginning\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X65tNaoOUbDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2583e511-301e-42ba-fc85-6802f0a21e73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Libraries imported successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import time\n",
        "from typing import Tuple, Dict, Any, Optional, List\n",
        "from tqdm.notebook import tqdm\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"‚úì Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcYMPnipUbDC"
      },
      "source": [
        "## 2. Embed ClinicEnv (Custom Gymnasium Environment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E41nLx6iUbDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0cbff78-c709-44d0-d3b9-ca093e4656e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì ClinicEnv defined successfully!\n"
          ]
        }
      ],
      "source": [
        "class ClinicEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Dermatology Clinic Triage Environment.\n",
        "\n",
        "    Observation Space (15 dimensions):\n",
        "        [0] age_norm, [1] duration_norm, [2] fever_flag, [3] infection_flag,\n",
        "        [4-11] symptom_embed (8-dim), [12] room_avail, [13] queue_len_norm, [14] time_norm\n",
        "\n",
        "    Action Space (8 discrete actions):\n",
        "        0: send_doctor, 1: send_nurse, 2: remote_advice, 3: escalate_priority,\n",
        "        4: defer_patient, 5: idle, 6: open_room, 7: close_room\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\", \"ansi\"], \"render_fps\": 6}\n",
        "\n",
        "    SEVERITY_MILD = 0\n",
        "    SEVERITY_MODERATE = 1\n",
        "    SEVERITY_SEVERE = 2\n",
        "    SEVERITY_CRITICAL = 3\n",
        "\n",
        "    def __init__(self, seed: Optional[int] = None, max_steps: int = 500, render_mode: Optional[str] = None):\n",
        "        super().__init__()\n",
        "        self.max_steps = max_steps\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        # Spaces\n",
        "        obs_low = np.array([0.0] * 15, dtype=np.float32)\n",
        "        obs_high = np.array([1.0] * 15, dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(obs_low, obs_high, dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(8)\n",
        "\n",
        "        # State\n",
        "        self.step_count = 0\n",
        "        self.num_open_rooms = 1\n",
        "        self.queue = []\n",
        "        self.current_patient = None\n",
        "        self.total_wait = 0.0\n",
        "\n",
        "        # Stats\n",
        "        self.episode_stats = {\n",
        "            \"correct_triages\": 0, \"incorrect_triages\": 0,\n",
        "            \"total_patients\": 0, \"total_wait_time\": 0.0, \"total_reward\": 0.0\n",
        "        }\n",
        "\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "        self.reset()\n",
        "\n",
        "    def seed(self, seed: Optional[int] = None):\n",
        "        self._seed = seed\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def _sample_patient(self) -> Dict[str, Any]:\n",
        "        severity = np.random.choice([0, 1, 2, 3], p=[0.4, 0.35, 0.2, 0.05])\n",
        "        age_norm = np.clip(np.random.normal(0.5, 0.15), 0.0, 1.0)\n",
        "        duration_norm = np.clip(np.random.exponential(0.5), 0.0, 1.0)\n",
        "        fever_flag = 1.0 if np.random.rand() < (0.05 + 0.2 * severity) else 0.0\n",
        "        infection_flag = 1.0 if np.random.rand() < (0.05 + 0.25 * severity) else 0.0\n",
        "        base = 0.2 + 0.25 * severity\n",
        "        symptom_embed = np.clip(np.random.normal(loc=base, scale=0.08, size=(8,)), 0.0, 1.0)\n",
        "        return {\n",
        "            \"severity\": int(severity), \"age_norm\": float(age_norm),\n",
        "            \"duration_norm\": float(duration_norm), \"fever_flag\": float(fever_flag),\n",
        "            \"infection_flag\": float(infection_flag), \"symptom_embed\": symptom_embed,\n",
        "            \"wait_time\": 0.0\n",
        "        }\n",
        "\n",
        "    def _form_observation(self, patient: Dict[str, Any]) -> np.ndarray:\n",
        "        vec = [patient[\"age_norm\"], patient[\"duration_norm\"],\n",
        "               patient[\"fever_flag\"], patient[\"infection_flag\"]]\n",
        "        vec += list(patient[\"symptom_embed\"])\n",
        "        vec += [1.0 if self.num_open_rooms > 0 else 0.0,\n",
        "                np.clip(len(self.queue) / 10.0, 0.0, 1.0),\n",
        "                np.clip(self.step_count / self.max_steps, 0.0, 1.0)]\n",
        "        return np.array(vec, dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "\n",
        "        self.step_count = 0\n",
        "        self.num_open_rooms = 1\n",
        "        self.queue = [self._sample_patient() for _ in range(3)]\n",
        "        self.current_patient = None\n",
        "        self.total_wait = 0.0\n",
        "        self.episode_stats = {\n",
        "            \"correct_triages\": 0, \"incorrect_triages\": 0,\n",
        "            \"total_patients\": 0, \"total_wait_time\": 0.0, \"total_reward\": 0.0\n",
        "        }\n",
        "\n",
        "        self._maybe_spawn_next()\n",
        "        obs = self._form_observation(self.current_patient)\n",
        "        info = self._get_info()\n",
        "        return obs, info\n",
        "\n",
        "    def _maybe_spawn_next(self):\n",
        "        if self.current_patient is None and len(self.queue) > 0:\n",
        "            self.current_patient = self.queue.pop(0)\n",
        "        elif self.current_patient is None:\n",
        "            self.current_patient = self._sample_patient()\n",
        "\n",
        "    def _get_correct_action(self, severity: int) -> int:\n",
        "        if severity == 0: return 2      # mild -> remote\n",
        "        elif severity == 1: return 1    # moderate -> nurse\n",
        "        elif severity == 2: return 0    # severe -> doctor\n",
        "        else: return 3                  # critical -> escalate\n",
        "\n",
        "    def _get_info(self) -> Dict[str, Any]:\n",
        "        if self.current_patient is None:\n",
        "            return {\"queue_length\": len(self.queue)}\n",
        "        return {\n",
        "            \"current_severity\": int(self.current_patient[\"severity\"]),\n",
        "            \"correct_action\": int(self._get_correct_action(self.current_patient[\"severity\"])),\n",
        "            \"num_open_rooms\": int(self.num_open_rooms),\n",
        "            \"queue_length\": len(self.queue),\n",
        "            \"episode_stats\": self.episode_stats.copy()\n",
        "        }\n",
        "\n",
        "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:\n",
        "        assert self.action_space.contains(action), f\"Invalid action: {action}\"\n",
        "\n",
        "        self.step_count += 1\n",
        "        patient = self.current_patient\n",
        "        reward = 0.0\n",
        "\n",
        "        correct_action = self._get_correct_action(patient[\"severity\"])\n",
        "\n",
        "        if action == correct_action:\n",
        "            if patient[\"severity\"] == 0:\n",
        "                reward += 1.0\n",
        "            elif patient[\"severity\"] == 1:\n",
        "                reward += 1.0\n",
        "            elif patient[\"severity\"] == 2:\n",
        "                reward += 2.0\n",
        "            else:\n",
        "                reward += 3.0 if patient[\"wait_time\"] < 5.0 else 2.0\n",
        "            self.episode_stats[\"correct_triages\"] += 1\n",
        "        else:\n",
        "            reward -= 1.5\n",
        "            self.episode_stats[\"incorrect_triages\"] += 1\n",
        "\n",
        "        if action == 6:\n",
        "            self.num_open_rooms += 1\n",
        "        elif action == 7 and self.num_open_rooms > 0:\n",
        "            self.num_open_rooms -= 1\n",
        "        elif action == 4:\n",
        "            patient[\"wait_time\"] += 1.0\n",
        "            self.queue.append(patient)\n",
        "            self.current_patient = None\n",
        "        else:\n",
        "            self.current_patient = None\n",
        "\n",
        "        wait_increment = 0.01 * len(self.queue)\n",
        "        for p in self.queue:\n",
        "            p[\"wait_time\"] += 1.0\n",
        "        self.total_wait += wait_increment\n",
        "        reward -= 0.01 * wait_increment\n",
        "        reward -= 0.05 * self.num_open_rooms\n",
        "\n",
        "        self._maybe_spawn_next()\n",
        "        self.episode_stats[\"total_patients\"] += 1\n",
        "        self.episode_stats[\"total_wait_time\"] += wait_increment\n",
        "        self.episode_stats[\"total_reward\"] += reward\n",
        "\n",
        "        obs = self._form_observation(self.current_patient)\n",
        "        terminated = False\n",
        "        truncated = self.step_count >= self.max_steps\n",
        "        info = self._get_info()\n",
        "\n",
        "        return obs, float(reward), terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "print(\"‚úì ClinicEnv defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF9auU7VUbDE"
      },
      "source": [
        "## 3. Load PPO Configurations (10 Hyperparameter Sets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bLa8aCUnUbDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d573bbb2-13e4-479a-deea-0ae50d9c2d01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Loaded 10 PPO configurations\n",
            "  - ppo_baseline: Baseline PPO configuration\n",
            "  - ppo_high_lr: Higher learning rate for faster learning\n",
            "  - ppo_low_lr: Lower learning rate for stability\n",
            "  - ppo_long_horizon: Longer rollout for better credit assignment\n",
            "  - ppo_high_entropy: Higher entropy coefficient for more exploration\n",
            "  - ppo_no_entropy: No entropy bonus for pure exploitation\n",
            "  - ppo_tight_clip: Tighter clipping for conservative updates\n",
            "  - ppo_wide_clip: Wider clipping for aggressive updates\n",
            "  - ppo_high_gae: Higher GAE lambda for less bias\n",
            "  - ppo_many_epochs: More epochs for better data utilization\n"
          ]
        }
      ],
      "source": [
        "# Embedded PPO configurations\n",
        "PPO_CONFIGS = {\n",
        "  \"configs\": [\n",
        "    {\n",
        "      \"id\": \"ppo_baseline\",\n",
        "      \"description\": \"Baseline PPO configuration\",\n",
        "      \"learning_rate\": 0.0003,\n",
        "      \"gamma\": 0.99,\n",
        "      \"n_steps\": 256,\n",
        "      \"batch_size\": 64,\n",
        "      \"n_epochs\": 10,\n",
        "      \"gae_lambda\": 0.95,\n",
        "      \"clip_range\": 0.2,\n",
        "      \"ent_coef\": 0.01,\n",
        "      \"vf_coef\": 0.5,\n",
        "      \"max_grad_norm\": 0.5\n",
        "    },\n",
        "    {\n",
        "      \"id\": \"ppo_high_lr\",\n",
        "      \"description\": \"Higher learning rate for faster learning\",\n",
        "      \"learning_rate\": 0.0005,\n",
        "      \"gamma\": 0.99,\n",
        "      \"n_steps\": 256,\n",
        "      \"batch_size\": 64,\n",
        "      \"n_epochs\": 10,\n",
        "      \"gae_lambda\": 0.95,\n",
        "      \"clip_range\": 0.2,\n",
        "      \"ent_coef\": 0.01,\n",
        "      \"vf_coef\": 0.5,\n",
        "      \"max_grad_norm\": 0.5\n",
        "    },\n",
        "    {\n",
        "      \"id\": \"ppo_low_lr\",\n",
        "      \"description\": \"Lower learning rate for stability\",\n",
        "      \"learning_rate\": 0.0001,\n",
        "      \"gamma\": 0.99,\n",
        "      \"n_steps\": 256,\n",
        "      \"batch_size\": 64,\n",
        "      \"n_epochs\": 10,\n",
        "      \"gae_lambda\": 0.95,\n",
        "      \"clip_range\": 0.2,\n",
        "      \"ent_coef\": 0.01,\n",
        "      \"vf_coef\": 0.5,\n",
        "      \"max_grad_norm\": 0.5\n",
        "    },\n",
        "    {\n",
        "      \"id\": \"ppo_long_horizon\",\n",
        "      \"description\": \"Longer rollout for better credit assignment\",\n",
        "      \"learning_rate\": 0.0003,\n",
        "      \"gamma\": 0.99,\n",
        "      \"n_steps\": 512,\n",
        "      \"batch_size\": 128,\n",
        "      \"n_epochs\": 10,\n",
        "      \"gae_lambda\": 0.95,\n",
        "      \"clip_range\": 0.2,\n",
        "      \"ent_coef\": 0.01,\n",
        "      \"vf_coef\": 0.5,\n",
        "      \"max_grad_norm\": 0.5\n",
        "    },\n",
        "    {\n",
        "      \"id\": \"ppo_high_entropy\",\n",
        "      \"description\": \"Higher entropy coefficient for more exploration\",\n",
        "      \"learning_rate\": 0.0003,\n",
        "      \"gamma\": 0.99,\n",
        "      \"n_steps\": 256,\n",
        "      \"batch_size\": 64,\n",
        "      \"n_epochs\": 10,\n",
        "      \"gae_lambda\": 0.95,\n",
        "      \"clip_range\": 0.2,\n",
        "      \"ent_coef\": 0.05,\n",
        "      \"vf_coef\": 0.5,\n",
        "      \"max_grad_norm\": 0.5\n",
        "    },\n",
        "    {\n",
        "      \"id\": \"ppo_no_entropy\",\n",
        "      \"description\": \"No entropy bonus for pure exploitation\",\n",
        "      \"learning_rate\": 0.0003,\n",
        "      \"gamma\": 0.99,\n",
        "      \"n_steps\": 256,\n",
        "      \"batch_size\": 64,\n",
        "      \"n_epochs\": 10,\n",
        "      \"gae_lambda\": 0.95,\n",
        "      \"clip_range\": 0.2,\n",
        "      \"ent_coef\": 0.0,\n",
        "      \"vf_coef\": 0.5,\n",
        "      \"max_grad_norm\": 0.5\n",
        "    },\n",
        "    {\n",
        "      \"id\": \"ppo_tight_clip\",\n",
        "      \"description\": \"Tighter clipping for conservative updates\",\n",
        "      \"learning_rate\": 0.0003,\n",
        "      \"gamma\": 0.99,\n",
        "      \"n_steps\": 256,\n",
        "      \"batch_size\": 64,\n",
        "      \"n_epochs\": 10,\n",
        "      \"gae_lambda\": 0.95,\n",
        "      \"clip_range\": 0.1,\n",
        "      \"ent_coef\": 0.01,\n",
        "      \"vf_coef\": 0.5,\n",
        "      \"max_grad_norm\": 0.5\n",
        "    },\n",
        "    {\n",
        "      \"id\": \"ppo_wide_clip\",\n",
        "      \"description\": \"Wider clipping for aggressive updates\",\n",
        "      \"learning_rate\": 0.0003,\n",
        "      \"gamma\": 0.99,\n",
        "      \"n_steps\": 256,\n",
        "      \"batch_size\": 64,\n",
        "      \"n_epochs\": 10,\n",
        "      \"gae_lambda\": 0.95,\n",
        "      \"clip_range\": 0.3,\n",
        "      \"ent_coef\": 0.01,\n",
        "      \"vf_coef\": 0.5,\n",
        "      \"max_grad_norm\": 0.5\n",
        "    },\n",
        "    {\n",
        "      \"id\": \"ppo_high_gae\",\n",
        "      \"description\": \"Higher GAE lambda for less bias\",\n",
        "      \"learning_rate\": 0.0003,\n",
        "      \"gamma\": 0.99,\n",
        "      \"n_steps\": 256,\n",
        "      \"batch_size\": 64,\n",
        "      \"n_epochs\": 10,\n",
        "      \"gae_lambda\": 0.98,\n",
        "      \"ent_coef\": 0.01,\n",
        "      \"vf_coef\": 0.5,\n",
        "      \"max_grad_norm\": 0.5\n",
        "    },\n",
        "    {\n",
        "      \"id\": \"ppo_many_epochs\",\n",
        "      \"description\": \"More epochs for better data utilization\",\n",
        "      \"learning_rate\": 0.0003,\n",
        "      \"gamma\": 0.99,\n",
        "      \"n_steps\": 256,\n",
        "      \"batch_size\": 64,\n",
        "      \"n_epochs\": 20,\n",
        "      \"gae_lambda\": 0.95,\n",
        "      \"clip_range\": 0.2,\n",
        "      \"ent_coef\": 0.01,\n",
        "      \"vf_coef\": 0.5,\n",
        "      \"max_grad_norm\": 0.5\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "configs = PPO_CONFIGS[\"configs\"]\n",
        "print(f\"‚úì Loaded {len(configs)} PPO configurations\")\n",
        "for cfg in configs:\n",
        "    print(f\"  - {cfg['id']}: {cfg['description']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jY9ZSwGhUbDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b2a4d0-0962-40da-aaac-d55ca44f8288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Helper functions defined!\n"
          ]
        }
      ],
      "source": [
        "def evaluate_agent(model, env, num_episodes=20, deterministic=True):\n",
        "    \"\"\"Evaluate trained agent and return metrics.\"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    triage_accuracies = []\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        obs, info = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "        episode_length = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=deterministic)\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            episode_reward += reward\n",
        "            episode_length += 1\n",
        "\n",
        "            if 'correct_action' in info:\n",
        "                total += 1\n",
        "                if action == info['correct_action']:\n",
        "                    correct += 1\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(episode_length)\n",
        "        if total > 0:\n",
        "            triage_accuracies.append(100.0 * correct / total)\n",
        "\n",
        "    return {\n",
        "        \"mean_reward\": np.mean(episode_rewards),\n",
        "        \"std_reward\": np.std(episode_rewards),\n",
        "        \"mean_length\": np.mean(episode_lengths),\n",
        "        \"mean_triage_accuracy\": np.mean(triage_accuracies) if triage_accuracies else 0.0,\n",
        "        \"std_triage_accuracy\": np.std(triage_accuracies) if triage_accuracies else 0.0\n",
        "    }\n",
        "\n",
        "def train_ppo_config(config, env, total_timesteps=50000, seed=42, verbose=False):\n",
        "    \"\"\"Train PPO with given configuration.\"\"\"\n",
        "    env.reset(seed=seed)\n",
        "\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        learning_rate=config[\"learning_rate\"],\n",
        "        gamma=config[\"gamma\"],\n",
        "        n_steps=config[\"n_steps\"],\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        n_epochs=config[\"n_epochs\"],\n",
        "        gae_lambda=config[\"gae_lambda\"],\n",
        "        clip_range=config[\"clip_range\"],\n",
        "        ent_coef=config[\"ent_coef\"],\n",
        "        vf_coef=config[\"vf_coef\"],\n",
        "        max_grad_norm=config[\"max_grad_norm\"],\n",
        "        seed=seed,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "    # Evaluate\n",
        "    eval_results = evaluate_agent(model, env, num_episodes=20)\n",
        "\n",
        "    return model, eval_results\n",
        "\n",
        "def save_results_to_csv(results, filename):\n",
        "    \"\"\"Save training results to CSV.\"\"\"\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"‚úì Results saved to {filename}\")\n",
        "\n",
        "def plot_config_comparison(results_df, save_path=None):\n",
        "    \"\"\"Plot comparison of all configurations.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Mean reward comparison\n",
        "    ax1 = axes[0]\n",
        "    sorted_df = results_df.sort_values('mean_reward', ascending=False)\n",
        "    bars = ax1.barh(sorted_df['config_id'], sorted_df['mean_reward'],\n",
        "                    color='steelblue', alpha=0.8)\n",
        "    ax1.set_xlabel('Mean Reward', fontsize=12)\n",
        "    ax1.set_title('Configuration Performance Comparison', fontsize=14, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    # Triage accuracy comparison\n",
        "    ax2 = axes[1]\n",
        "    bars = ax2.barh(sorted_df['config_id'], sorted_df['triage_accuracy'],\n",
        "                    color='coral', alpha=0.8)\n",
        "    ax2.set_xlabel('Triage Accuracy (%)', fontsize=12)\n",
        "    ax2.set_title('Triage Accuracy by Configuration', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"‚úì Plot saved to {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úì Helper functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ4KfPNCUbDH"
      },
      "source": [
        "## 5. Quick Sweep: Train All 10 Configs (50K steps each)\n",
        "\n",
        "**Purpose**: Identify the best-performing configuration before committing to full training.\n",
        "\n",
        "**Runtime**: ~2-3 hours on Colab GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsM5k_d6UbDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c808dfb-69b4-4abe-8197-4614a6c42013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "QUICK SWEEP: Training all 10 configurations\n",
            "============================================================\n",
            "\n",
            "[1/10] Training: ppo_baseline\n",
            "Description: Baseline PPO configuration\n",
            "------------------------------------------------------------\n",
            "‚úó Error training ppo_baseline: 'buffer_size'\n",
            "\n",
            "[2/10] Training: ppo_high_lr\n",
            "Description: Higher learning rate for faster learning\n",
            "------------------------------------------------------------\n",
            "‚úó Error training ppo_high_lr: 'buffer_size'\n",
            "\n",
            "[3/10] Training: ppo_low_lr\n",
            "Description: Lower learning rate for stability\n",
            "------------------------------------------------------------\n",
            "‚úó Error training ppo_low_lr: 'buffer_size'\n",
            "\n",
            "[4/10] Training: ppo_long_horizon\n",
            "Description: Longer rollout for better credit assignment\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Quick sweep training\n",
        "print(\"=\"*60)\n",
        "print(\"QUICK SWEEP: Training all 10 configurations\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sweep_results = []\n",
        "sweep_models = {}\n",
        "\n",
        "for i, config in enumerate(configs):\n",
        "    print(f\"\\n[{i+1}/10] Training: {config['id']}\")\n",
        "    print(f\"Description: {config['description']}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Create environment\n",
        "    env = ClinicEnv(seed=42, max_steps=500)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train\n",
        "    try:\n",
        "        model, eval_results = train_ppo_config(\n",
        "            config, env,\n",
        "            total_timesteps=100000,\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            \"config_id\": config[\"id\"],\n",
        "            \"mean_reward\": eval_results[\"mean_reward\"],\n",
        "            \"std_reward\": eval_results[\"std_reward\"],\n",
        "            \"triage_accuracy\": eval_results[\"mean_triage_accuracy\"],\n",
        "            \"mean_length\": eval_results[\"mean_length\"],\n",
        "            \"learning_rate\": config[\"learning_rate\"],\n",
        "            \"gamma\": config[\"gamma\"],\n",
        "            \"buffer_size\": config[\"buffer_size\"],\n",
        "            \"batch_size\": config[\"batch_size\"],\n",
        "            \"training_time_sec\": elapsed\n",
        "        }\n",
        "\n",
        "        sweep_results.append(result)\n",
        "        sweep_models[config[\"id\"]] = model\n",
        "\n",
        "        print(f\"‚úì Completed in {elapsed:.1f}s\")\n",
        "        print(f\"  Mean Reward: {eval_results['mean_reward']:.2f} ¬± {eval_results['std_reward']:.2f}\")\n",
        "        print(f\"  Triage Accuracy: {eval_results['mean_triage_accuracy']:.1f}%\")\n",
        "\n",
        "        # Save model to Drive\n",
        "        model_path = f\"{PROJECT_DIR}/models/ppo/{config['id']}_sweep.zip\"\n",
        "        model.save(model_path)\n",
        "        print(f\"  Model saved: {model_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error training {config['id']}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "    env.close()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SWEEP COMPLETE!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAPeo894UbDH"
      },
      "source": [
        "## 6. Analyze Results & Select Best Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiTJD24KUbDH"
      },
      "outputs": [],
      "source": [
        "# Create results dataframe\n",
        "sweep_df = pd.DataFrame(sweep_results)\n",
        "\n",
        "# Save sweep results\n",
        "sweep_csv_path = f\"{PROJECT_DIR}/results/ppo_sweep_results.csv\"\n",
        "save_results_to_csv(sweep_results, sweep_csv_path)\n",
        "\n",
        "# Display sorted results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SWEEP RESULTS (sorted by mean reward)\")\n",
        "print(\"=\"*60)\n",
        "print(sweep_df.sort_values('mean_reward', ascending=False)[\n",
        "    ['config_id', 'mean_reward', 'triage_accuracy', 'training_time_sec']\n",
        "].to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Plot comparison\n",
        "plot_config_comparison(\n",
        "    sweep_df,\n",
        "    save_path=f\"{PROJECT_DIR}/plots/ppo_sweep_comparison.png\"\n",
        ")\n",
        "\n",
        "# Identify best config\n",
        "best_config_id = sweep_df.loc[sweep_df['mean_reward'].idxmax(), 'config_id']\n",
        "best_config = next(c for c in configs if c['id'] == best_config_id)\n",
        "\n",
        "print(f\"\\nüèÜ BEST CONFIGURATION: {best_config_id}\")\n",
        "print(f\"   Mean Reward: {sweep_df.loc[sweep_df['mean_reward'].idxmax(), 'mean_reward']:.2f}\")\n",
        "print(f\"   Triage Accuracy: {sweep_df.loc[sweep_df['mean_reward'].idxmax(), 'triage_accuracy']:.1f}%\")\n",
        "print(f\"\\n   Hyperparameters:\")\n",
        "for key in ['learning_rate', 'gamma', 'buffer_size', 'batch_size']:\n",
        "    print(f\"     {key}: {best_config[key]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu5VufVeUbDK"
      },
      "source": [
        "## 7. Full Training: Best Config with 5 Seeds (200K steps each)\n",
        "\n",
        "**Purpose**: Train the best configuration with multiple seeds for statistical significance.\n",
        "\n",
        "**Runtime**: ~3-4 hours on Colab GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiyX_CcfUbDK"
      },
      "outputs": [],
      "source": [
        "# Full training with multiple seeds\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"FULL TRAINING: {best_config_id} with 5 seeds\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "full_results = []\n",
        "full_models = {}\n",
        "SEEDS = [42, 123, 456, 789, 1024]\n",
        "\n",
        "for i, seed in enumerate(SEEDS):\n",
        "    print(f\"\\n[Seed {i+1}/5] Training with seed={seed}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Create environment\n",
        "    env = ClinicEnv(seed=seed, max_steps=500)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        model, eval_results = train_ppo_config(\n",
        "            best_config, env,\n",
        "            total_timesteps=200000,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            \"config_id\": best_config_id,\n",
        "            \"seed\": seed,\n",
        "            \"mean_reward\": eval_results[\"mean_reward\"],\n",
        "            \"std_reward\": eval_results[\"std_reward\"],\n",
        "            \"triage_accuracy\": eval_results[\"mean_triage_accuracy\"],\n",
        "            \"triage_accuracy_std\": eval_results[\"std_triage_accuracy\"],\n",
        "            \"mean_length\": eval_results[\"mean_length\"],\n",
        "            \"training_time_sec\": elapsed\n",
        "        }\n",
        "\n",
        "        full_results.append(result)\n",
        "        full_models[f\"seed_{seed}\"] = model\n",
        "\n",
        "        print(f\"‚úì Completed in {elapsed:.1f}s\")\n",
        "        print(f\"  Mean Reward: {eval_results['mean_reward']:.2f} ¬± {eval_results['std_reward']:.2f}\")\n",
        "        print(f\"  Triage Accuracy: {eval_results['mean_triage_accuracy']:.1f}% ¬± {eval_results['std_triage_accuracy']:.1f}%\")\n",
        "\n",
        "        # Save model to Drive\n",
        "        model_path = f\"{PROJECT_DIR}/models/ppo/{best_config_id}_seed{seed}.zip\"\n",
        "        model.save(model_path)\n",
        "        print(f\"  Model saved: {model_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error training seed {seed}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "    env.close()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FULL TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVPrZablUbDe"
      },
      "source": [
        "## 8. Final Results & Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7gx9zypUbDe"
      },
      "outputs": [],
      "source": [
        "# Aggregate full training results\n",
        "full_df = pd.DataFrame(full_results)\n",
        "\n",
        "# Save full results\n",
        "full_csv_path = f\"{PROJECT_DIR}/results/ppo_full_results.csv\"\n",
        "save_results_to_csv(full_results, full_csv_path)\n",
        "\n",
        "# Calculate statistics across seeds\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS (aggregated across 5 seeds)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Configuration: {best_config_id}\")\n",
        "print(f\"Total Timesteps: 200,000 per seed √ó 5 seeds = 1,000,000\")\n",
        "print()\n",
        "print(f\"Mean Reward: {full_df['mean_reward'].mean():.2f} ¬± {full_df['mean_reward'].std():.2f}\")\n",
        "print(f\"Triage Accuracy: {full_df['triage_accuracy'].mean():.1f}% ¬± {full_df['triage_accuracy'].std():.1f}%\")\n",
        "print(f\"Avg Episode Length: {full_df['mean_length'].mean():.1f}\")\n",
        "print(f\"Total Training Time: {full_df['training_time_sec'].sum()/3600:.2f} hours\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Plot full training results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Reward distribution\n",
        "ax1 = axes[0]\n",
        "ax1.bar(range(len(SEEDS)), full_df['mean_reward'],\n",
        "        yerr=full_df['std_reward'], capsize=5, alpha=0.7, color='steelblue')\n",
        "ax1.axhline(y=full_df['mean_reward'].mean(), color='red', linestyle='--',\n",
        "            label=f\"Mean: {full_df['mean_reward'].mean():.2f}\")\n",
        "ax1.set_xlabel('Seed Index', fontsize=12)\n",
        "ax1.set_ylabel('Mean Reward', fontsize=12)\n",
        "ax1.set_title('Reward Across Seeds', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Triage accuracy distribution\n",
        "ax2 = axes[1]\n",
        "ax2.bar(range(len(SEEDS)), full_df['triage_accuracy'],\n",
        "        yerr=full_df['triage_accuracy_std'], capsize=5, alpha=0.7, color='coral')\n",
        "ax2.axhline(y=full_df['triage_accuracy'].mean(), color='red', linestyle='--',\n",
        "            label=f\"Mean: {full_df['triage_accuracy'].mean():.1f}%\")\n",
        "ax2.set_xlabel('Seed Index', fontsize=12)\n",
        "ax2.set_ylabel('Triage Accuracy (%)', fontsize=12)\n",
        "ax2.set_title('Triage Accuracy Across Seeds', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "full_plot_path = f\"{PROJECT_DIR}/plots/ppo_full_training.png\"\n",
        "plt.savefig(full_plot_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"\\n‚úì Plot saved to {full_plot_path}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSLkzgZEUbDe"
      },
      "source": [
        "## 9. Save Best Model & Export Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5AYwHD7UbDe"
      },
      "outputs": [],
      "source": [
        "# Identify best model from full training\n",
        "best_seed_idx = full_df['mean_reward'].idxmax()\n",
        "best_seed = full_df.loc[best_seed_idx, 'seed']\n",
        "best_model = full_models[f\"seed_{best_seed}\"]\n",
        "\n",
        "# Save best model\n",
        "best_model_path = f\"{PROJECT_DIR}/models/ppo/best_model.zip\"\n",
        "best_model.save(best_model_path)\n",
        "print(f\"‚úì Best model saved to {best_model_path}\")\n",
        "print(f\"  (seed={best_seed}, reward={full_df.loc[best_seed_idx, 'mean_reward']:.2f})\")\n",
        "\n",
        "# Create summary JSON\n",
        "summary = {\n",
        "    \"algorithm\": \"DQN\",\n",
        "    \"best_config_id\": best_config_id,\n",
        "    \"best_config\": best_config,\n",
        "    \"quick_sweep\": {\n",
        "        \"num_configs\": len(configs),\n",
        "        \"timesteps_per_config\": 50000,\n",
        "        \"best_mean_reward\": float(sweep_df['mean_reward'].max()),\n",
        "        \"best_triage_accuracy\": float(sweep_df.loc[sweep_df['mean_reward'].idxmax(), 'triage_accuracy'])\n",
        "    },\n",
        "    \"full_training\": {\n",
        "        \"num_seeds\": len(SEEDS),\n",
        "        \"timesteps_per_seed\": 200000,\n",
        "        \"mean_reward\": float(full_df['mean_reward'].mean()),\n",
        "        \"std_reward\": float(full_df['mean_reward'].std()),\n",
        "        \"mean_triage_accuracy\": float(full_df['triage_accuracy'].mean()),\n",
        "        \"std_triage_accuracy\": float(full_df['triage_accuracy'].std()),\n",
        "        \"best_seed\": int(best_seed),\n",
        "        \"best_seed_reward\": float(full_df.loc[best_seed_idx, 'mean_reward'])\n",
        "    },\n",
        "    \"best_model_path\": best_model_path\n",
        "}\n",
        "\n",
        "summary_path = f\"{PROJECT_DIR}/results/ppo_summary.json\"\n",
        "with open(summary_path, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"‚úì Summary saved to {summary_path}\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PPO TRAINING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(json.dumps(summary, indent=2))\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zywoPhh2UbDe"
      },
      "source": [
        "## ‚úÖ Training Complete!\n",
        "\n",
        "All results have been saved to Google Drive:\n",
        "\n",
        "- **Models**: `{PROJECT_DIR}/models/dqn/`\n",
        "- **Results**: `{PROJECT_DIR}/results/`\n",
        "- **Plots**: `{PROJECT_DIR}/plots/`\n",
        "\n",
        "### Next Steps:\n",
        "1. Repeat this process for PPO, A2C, and REINFORCE\n",
        "2. Compare all algorithms using `aggregate_results.py`\n",
        "3. Select best overall algorithm for extended training\n",
        "4. Generate demo videos for report\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}